{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-8xl23O4TPWq",
        "outputId": "d8243011-6ade-48d2-d256-478ab615a113"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Israel count: 500\n",
            "Iran count: 453\n",
            "Netanyahu count: 112\n",
            "Khamenei count: 30\n"
          ]
        }
      ],
      "source": [
        "# Initialize counters for the words \"Israel\" and \"Iran\"\n",
        "israel_count = 0\n",
        "iran_count = 0\n",
        "netanyahu_count = 0\n",
        "khamenei_count = 0\n",
        "# Open the file in read mode\n",
        "with open('/content/News Stories - Israel Iran (1).txt', 'r') as file:\n",
        "    # Read each line in the file\n",
        "    for line in file:\n",
        "        # Split the line into words\n",
        "        words = line.split()\n",
        "        # Loop through each word\n",
        "        for word in words:\n",
        "            # Check if the word is \"Israel\" or \"Iran\" (case-insensitive)\n",
        "            if word.lower() == \"israel\":\n",
        "                israel_count += 1\n",
        "            elif word.lower() == \"iran\":\n",
        "                iran_count += 1\n",
        "            elif word.lower() == \"netanyahu\":\n",
        "              netanyahu_count += 2\n",
        "            elif word.lower() == \"khamenei\":\n",
        "              khamenei_count += 2\n",
        "\n",
        "# Print the counts\n",
        "print(\"Israel count:\", israel_count)\n",
        "print(\"Iran count:\", iran_count)\n",
        "print(\"Netanyahu count:\", netanyahu_count)\n",
        "print(\"Khamenei count:\", khamenei_count)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "import nltk\n",
        "\n",
        "# Download the VADER lexicon (if not already downloaded)\n",
        "nltk.download('vader_lexicon')\n",
        "\n",
        "# Create a DataFrame from the sample data\n",
        "df = pd.read_csv(\"/content/data.csv\", encoding='ISO-8859-1')\n",
        "\n",
        "# Filter the data to only include rows where the country is Israel or Iran\n",
        "df = df[df['country'].isin(['Israel', 'Iran'])]\n",
        "\n",
        "# Initialize the VADER sentiment analyzer\n",
        "sid = SentimentIntensityAnalyzer()\n",
        "\n",
        "# Function to calculate sentiment scores for each sentence\n",
        "def get_sentiment_scores(text):\n",
        "    scores = sid.polarity_scores(text)\n",
        "    return scores\n",
        "\n",
        "# Set the sample size and number of iterations\n",
        "sample_size = 10\n",
        "num_iterations = 50\n",
        "\n",
        "# Initialize lists to store the sentiment scores for each iteration\n",
        "israel_scores_list = []\n",
        "iran_scores_list = []\n",
        "\n",
        "# Loop over the number of iterations\n",
        "for i in range(num_iterations):\n",
        "    # Shuffle the data and split it into two subsets\n",
        "    df = df.sample(frac=1).reset_index(drop=True)\n",
        "    israel_df = df[df['country'] == 'Israel']\n",
        "    iran_df = df[df['country'] == 'Iran']\n",
        "\n",
        "    # Sample 50 rows from each subset\n",
        "    israel_sampled_df = israel_df.sample(20, replace=True)\n",
        "    iran_sampled_df = iran_df.sample(20, replace=True)\n",
        "\n",
        "    # Concatenate the sampled subsets\n",
        "    sampled_df = pd.concat([israel_sampled_df, iran_sampled_df], ignore_index=True)\n",
        "\n",
        "    # Apply sentiment analysis to the text column\n",
        "    sampled_df['sentiment_scores'] = sampled_df['text'].apply(get_sentiment_scores)\n",
        "\n",
        "    # Calculate average sentiment scores for each subset\n",
        "    israel_avg_scores = {\n",
        "        'positive': sampled_df[sampled_df['country'] == 'Israel']['sentiment_scores'].apply(lambda x: x['pos']).mean(),\n",
        "        'neutral': sampled_df[sampled_df['country'] == 'Israel']['sentiment_scores'].apply(lambda x: x['neu']).mean(),\n",
        "        'negative': sampled_df[sampled_df['country'] == 'Israel']['sentiment_scores'].apply(lambda x: x['neg']).mean(),\n",
        "        'compound': sampled_df[sampled_df['country'] == 'Israel']['sentiment_scores'].apply(lambda x: x['compound']).mean()\n",
        "    }\n",
        "\n",
        "    iran_avg_scores = {\n",
        "        'positive': sampled_df[sampled_df['country'] == 'Iran']['sentiment_scores'].apply(lambda x: x['pos']).mean(),\n",
        "        'neutral': sampled_df[sampled_df['country'] == 'Iran']['sentiment_scores'].apply(lambda x: x['neu']).mean(),\n",
        "        'negative': sampled_df[sampled_df['country'] == 'Iran']['sentiment_scores'].apply(lambda x: x['neg']).mean(),\n",
        "        'compound': sampled_df[sampled_df['country'] == 'Iran']['sentiment_scores'].apply(lambda x: x['compound']).mean()\n",
        "    }\n",
        "\n",
        "    # Append the sentiment scores to the lists\n",
        "    israel_scores_list.append(israel_avg_scores)\n",
        "    iran_scores_list.append(iran_avg_scores)\n",
        "\n",
        "# Calculate the overall average sentiment scores\n",
        "israel_avg_scores = {\n",
        "    'positive': np.mean([x['positive'] for x in israel_scores_list]),\n",
        "    'neutral': np.mean([x['neutral'] for x in israel_scores_list]),\n",
        "    'negative': np.mean([x['negative'] for x in israel_scores_list]),\n",
        "    'compound': np.mean([x['compound'] for x in israel_scores_list])\n",
        "}\n",
        "\n",
        "iran_avg_scores = {\n",
        "    'positive': np.mean([x['positive'] for x in iran_scores_list]),\n",
        "    'neutral': np.mean([x['neutral'] for x in iran_scores_list]),\n",
        "    'negative': np.mean([x['negative'] for x in iran_scores_list]),\n",
        "    'compound': np.mean([x['compound'] for x in iran_scores_list])\n",
        "}\n",
        "\n",
        "# Print the average sentiment scores\n",
        "print(\"Israel Average Sentiment Scores:\")\n",
        "print(israel_avg_scores)\n",
        "print(\"\\nIran Average Sentiment Scores:\")\n",
        "print(iran_avg_scores)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qlvwuj2vwY1l",
        "outputId": "91344ded-e3bc-4d80-e2e6-d340b190ac92"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n",
            "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Israel Average Sentiment Scores:\n",
            "{'positive': 0.07553599999999999, 'neutral': 0.747396, 'negative': 0.17707899999999996, 'compound': -0.1878989}\n",
            "\n",
            "Iran Average Sentiment Scores:\n",
            "{'positive': 0.05185399999999998, 'neutral': 0.769006, 'negative': 0.179129, 'compound': -0.28728149999999997}\n"
          ]
        }
      ]
    }
  ]
}